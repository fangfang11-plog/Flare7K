2023-04-12 09:25:28,463 INFO: 
                ____                _       _____  ____
               / __ ) ____ _ _____ (_)_____/ ___/ / __ \
              / __  |/ __ `// ___// // ___/\__ \ / /_/ /
             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/
            /_____/ \__,_//____//_/ \___//____//_/ |_|
     ______                   __   __                 __      __
    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /
   / / __ / __ \ / __ \ / __  /  / /   / / / // ___// //_/  / /
  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/
  \____/ \____/ \____/ \____/  /_____/\____/ \___//_/|_|  (_)
    
Version Information: 
	BasicSR: 1.4.1
	PyTorch: 1.9.0+cu111
	TorchVision: 0.10.0+cu111
2023-04-12 09:25:28,463 INFO: 
  name: FlareRemoval_Uformer_6ch
  model_type: DeflareModel
  scale: 1
  num_gpu: 2
  manual_seed: 0
  datasets:[
    train:[
      name: Flare7K
      type: Flare_Pair_Loader
      image_path: dataset/Flickr24K
      scattering_dict:[
        Flare7K_scattering: dataset/Flare7k/Scattering_Flare/Compound_Flare
      ]
      reflective_dict:[
        Flare7K_reflective: dataset/Flare7k/Reflective_Flare
      ]
      transform_base:[
        img_size: 512
      ]
      transform_flare:[
        scale_min: 0.8
        scale_max: 1.5
        translate: 300
        shear: 20
      ]
      mask_type: None
      use_shuffle: True
      num_worker_per_gpu: 4
      batch_size_per_gpu: 1
      dataset_enlarge_ratio: 1
      prefetch_mode: None
      phase: train
      scale: 1
    ]
    val:[
      name: flare_test
      type: Image_Pair_Loader
      dataroot_gt: dataset/Flare7k/valid/gt
      dataroot_lq: dataset/Flare7k/valid/input
      gt_size: 512
      phase: val
      scale: 1
    ]
  ]
  network_g:[
    type: Uformer
    img_size: 512
    img_ch: 3
    output_ch: 6
    multi_stage: 1
  ]
  path:[
    pretrain_network_g: None
    strict_load_g: True
    resume_state: None
    experiments_root: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch
    models: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch/models
    training_states: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch/training_states
    log: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch
    visualization: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch/visualization
  ]
  train:[
    optim_g:[
      type: Adam
      lr: 0.0001
      weight_decay: 0
      betas: [0.9, 0.99]
    ]
    scheduler:[
      type: MultiStepLR
      milestones: [200000]
      gamma: 1.0
    ]
    ema_decay: 0
    total_iter: 1200000
    warmup_iter: -1
    l1_opt:[
      type: L_Abs_pure
      loss_weight: 0.5
    ]
    perceptual:[
      type: L_percepture
      loss_weight: 0.5
    ]
  ]
  val:[
    val_freq: 5000.0
    save_img: True
    metrics:[
      psnr:[
        type: calculate_psnr
        crop_border: 4
        test_y_channel: False
      ]
      ssim:[
        type: calculate_ssim
        crop_border: 4
        test_y_channel: False
      ]
    ]
  ]
  logger:[
    print_freq: 100
    save_checkpoint_freq: 50000.0
    use_tb_logger: True
    wandb:[
      project: None
      resume_id: None
    ]
  ]
  dist_params:[
    backend: nccl
    port: 29500
  ]
  dist: False
  rank: 0
  world_size: 1
  auto_resume: False
  is_train: True
  root_path: /home/fanglihuang/workspace/data/Flare7K

2023-04-12 09:25:28,798 INFO: Dataset [Flare_Pair_Loader] - Flare7K is built.
2023-04-12 09:25:28,799 INFO: Training statistics:
	Number of train images: 23949
	Dataset enlarge ratio: 1
	Batch size per gpu: 1
	World size (gpu number): 1
	Require iter number per epoch: 23949
	Total epochs: 51; iters: 1200000.
2023-04-12 09:25:28,799 INFO: Dataset [Image_Pair_Loader] - flare_test is built.
2023-04-12 09:25:28,799 INFO: Number of val images/folders in flare_test: 0
2023-04-12 09:25:29,713 INFO: Network [Uformer] is created.
2023-04-12 09:25:32,596 INFO: Network: DataParallel - Uformer, with parameters: 20,473,888
2023-04-12 09:25:32,596 INFO: Uformer(
  embed_dim=32, token_projection=linear, token_mlp=ffn,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(64, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=32, input_resolution=(512, 512), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(512, 512), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(512, 512), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.014)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=64, input_resolution=(256, 256), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(256, 256), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.029)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(256, 256), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.043)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(128, 128), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.057)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(128, 128), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.071)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=256, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(64, 64), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.086)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(64, 64), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=512, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(32, 32), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(32, 32), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=512, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.086)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=256, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(128, 128), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.071)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(128, 128), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.057)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(256, 256), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(256, 256), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.043)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(256, 256), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.029)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=64, input_resolution=(512, 512), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(512, 512), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.014)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(512, 512), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (activation): Sequential(
    (0): Sigmoid()
  )
)
2023-04-12 09:25:32,598 INFO: Loss [L_Abs_pure] is created.
2023-04-12 09:25:34,324 INFO: Loss [L_percepture] is created.
2023-04-12 09:25:34,326 INFO: Model [DeflareModel] is created.
2023-04-12 09:25:34,644 INFO: Start training from epoch: 0, iter: 0
2023-04-12 09:27:12,570 INFO: [Flare..][epoch:  0, iter:     100, lr:(1.000e-04,)] [eta: 11 days, 12:14:20, time (data): 0.979 (0.074)] l1_recons: 3.1535e-01 l1_flare: 1.6436e-01 l1_base: 1.0852e-01 l1: 5.8822e-01 l_vgg: 5.1541e+00 l_vgg_base: 3.9605e+00 l_vgg_flare: 1.1936e+00 
2023-04-12 09:28:38,753 INFO: [Flare..][epoch:  0, iter:     200, lr:(1.000e-04,)] [eta: 11 days, 17:41:39, time (data): 0.921 (0.040)] l1_recons: 1.8975e-01 l1_flare: 1.1155e-01 l1_base: 8.1548e-02 l1: 3.8285e-01 l_vgg: 4.9671e+00 l_vgg_base: 3.6562e+00 l_vgg_flare: 1.3108e+00 
2023-04-12 09:30:06,343 INFO: [Flare..][epoch:  0, iter:     300, lr:(1.000e-04,)] [eta: 11 days, 21:04:02, time (data): 0.876 (0.006)] l1_recons: 2.3721e-01 l1_flare: 5.9911e-02 l1_base: 1.0907e-01 l1: 4.0620e-01 l_vgg: 4.6527e+00 l_vgg_base: 3.3530e+00 l_vgg_flare: 1.2998e+00 
2023-04-12 09:31:34,565 INFO: [Flare..][epoch:  0, iter:     400, lr:(1.000e-04,)] [eta: 11 days, 23:16:14, time (data): 0.879 (0.006)] l1_recons: 1.8928e-01 l1_flare: 5.3071e-02 l1_base: 9.3816e-02 l1: 3.3617e-01 l_vgg: 4.5427e+00 l_vgg_base: 3.8496e+00 l_vgg_flare: 6.9306e-01 
2023-04-12 09:33:02,741 INFO: [Flare..][epoch:  0, iter:     500, lr:(1.000e-04,)] [eta: 12 days, 0:33:17, time (data): 0.882 (0.006)] l1_recons: 1.2404e-01 l1_flare: 4.7700e-02 l1_base: 6.9824e-02 l1: 2.4157e-01 l_vgg: 3.1825e+00 l_vgg_base: 2.5544e+00 l_vgg_flare: 6.2812e-01 
2023-04-12 09:34:30,615 INFO: [Flare..][epoch:  0, iter:     600, lr:(1.000e-04,)] [eta: 12 days, 1:14:07, time (data): 0.880 (0.006)] l1_recons: 8.8711e-02 l1_flare: 6.9998e-02 l1_base: 6.3506e-02 l1: 2.2222e-01 l_vgg: 4.1984e+00 l_vgg_base: 3.3521e+00 l_vgg_flare: 8.4625e-01 
2023-04-12 09:35:58,643 INFO: [Flare..][epoch:  0, iter:     700, lr:(1.000e-04,)] [eta: 12 days, 1:47:16, time (data): 0.881 (0.006)] l1_recons: 1.0661e-01 l1_flare: 4.6975e-02 l1_base: 5.7597e-02 l1: 2.1118e-01 l_vgg: 3.0994e+00 l_vgg_base: 2.5737e+00 l_vgg_flare: 5.2573e-01 
2023-04-12 09:37:26,802 INFO: [Flare..][epoch:  0, iter:     800, lr:(1.000e-04,)] [eta: 12 days, 2:15:05, time (data): 0.881 (0.006)] l1_recons: 1.0996e-01 l1_flare: 4.8955e-02 l1_base: 5.8874e-02 l1: 2.1779e-01 l_vgg: 3.4570e+00 l_vgg_base: 2.8544e+00 l_vgg_flare: 6.0268e-01 
2023-04-12 09:38:54,885 INFO: [Flare..][epoch:  0, iter:     900, lr:(1.000e-04,)] [eta: 12 days, 2:34:41, time (data): 0.881 (0.006)] l1_recons: 1.1713e-01 l1_flare: 6.7333e-02 l1_base: 8.6290e-02 l1: 2.7075e-01 l_vgg: 4.1616e+00 l_vgg_base: 3.3082e+00 l_vgg_flare: 8.5337e-01 
2023-04-12 09:40:22,820 INFO: [Flare..][epoch:  0, iter:   1,000, lr:(1.000e-04,)] [eta: 12 days, 2:47:07, time (data): 0.880 (0.006)] l1_recons: 1.5686e-01 l1_flare: 6.4610e-02 l1_base: 8.8798e-02 l1: 3.1027e-01 l_vgg: 3.7513e+00 l_vgg_base: 2.8654e+00 l_vgg_flare: 8.8584e-01 
2023-04-12 09:41:51,050 INFO: [Flare..][epoch:  0, iter:   1,100, lr:(1.000e-04,)] [eta: 12 days, 3:02:23, time (data): 0.883 (0.006)] l1_recons: 1.2062e-01 l1_flare: 2.6676e-02 l1_base: 6.1519e-02 l1: 2.0881e-01 l_vgg: 4.4381e+00 l_vgg_base: 3.3498e+00 l_vgg_flare: 1.0883e+00 
2023-04-12 09:43:19,322 INFO: [Flare..][epoch:  0, iter:   1,200, lr:(1.000e-04,)] [eta: 12 days, 3:15:34, time (data): 0.883 (0.006)] l1_recons: 1.4064e-01 l1_flare: 2.7817e-02 l1_base: 7.2741e-02 l1: 2.4120e-01 l_vgg: 3.8604e+00 l_vgg_base: 2.4955e+00 l_vgg_flare: 1.3649e+00 
2023-04-12 09:44:47,823 INFO: [Flare..][epoch:  0, iter:   1,300, lr:(1.000e-04,)] [eta: 12 days, 3:30:01, time (data): 0.884 (0.006)] l1_recons: 9.8343e-02 l1_flare: 6.3977e-02 l1_base: 5.9992e-02 l1: 2.2231e-01 l_vgg: 3.9784e+00 l_vgg_base: 2.8949e+00 l_vgg_flare: 1.0836e+00 
2023-04-12 09:46:15,937 INFO: [Flare..][epoch:  0, iter:   1,400, lr:(1.000e-04,)] [eta: 12 days, 3:36:40, time (data): 0.883 (0.006)] l1_recons: 7.9208e-02 l1_flare: 4.6416e-02 l1_base: 4.9501e-02 l1: 1.7512e-01 l_vgg: 3.6396e+00 l_vgg_base: 2.8297e+00 l_vgg_flare: 8.0991e-01 
2023-04-12 09:47:44,290 INFO: [Flare..][epoch:  0, iter:   1,500, lr:(1.000e-04,)] [eta: 12 days, 3:45:25, time (data): 0.884 (0.006)] l1_recons: 1.4833e-01 l1_flare: 4.6573e-02 l1_base: 7.6149e-02 l1: 2.7105e-01 l_vgg: 4.1754e+00 l_vgg_base: 3.3816e+00 l_vgg_flare: 7.9383e-01 
2023-04-12 09:49:12,624 INFO: [Flare..][epoch:  0, iter:   1,600, lr:(1.000e-04,)] [eta: 12 days, 3:52:39, time (data): 0.884 (0.006)] l1_recons: 8.4487e-02 l1_flare: 5.3033e-02 l1_base: 6.7416e-02 l1: 2.0494e-01 l_vgg: 3.5176e+00 l_vgg_base: 2.7440e+00 l_vgg_flare: 7.7361e-01 
2023-04-12 09:50:40,685 INFO: [Flare..][epoch:  0, iter:   1,700, lr:(1.000e-04,)] [eta: 12 days, 3:55:40, time (data): 0.880 (0.006)] l1_recons: 6.6116e-02 l1_flare: 1.6127e-02 l1_base: 3.5558e-02 l1: 1.1780e-01 l_vgg: 2.8296e+00 l_vgg_base: 2.4288e+00 l_vgg_flare: 4.0078e-01 
2023-04-12 09:52:08,708 INFO: [Flare..][epoch:  0, iter:   1,800, lr:(1.000e-04,)] [eta: 12 days, 3:57:45, time (data): 0.880 (0.006)] l1_recons: 7.2420e-02 l1_flare: 3.4206e-02 l1_base: 3.9150e-02 l1: 1.4578e-01 l_vgg: 3.3423e+00 l_vgg_base: 2.3444e+00 l_vgg_flare: 9.9796e-01 
2023-04-12 09:53:36,675 INFO: [Flare..][epoch:  0, iter:   1,900, lr:(1.000e-04,)] [eta: 12 days, 3:58:53, time (data): 0.880 (0.006)] l1_recons: 3.1456e-02 l1_flare: 4.1439e-02 l1_base: 1.9991e-02 l1: 9.2886e-02 l_vgg: 2.3625e+00 l_vgg_base: 1.2939e+00 l_vgg_flare: 1.0686e+00 
2023-04-12 09:55:04,695 INFO: [Flare..][epoch:  0, iter:   2,000, lr:(1.000e-04,)] [eta: 12 days, 4:00:16, time (data): 0.880 (0.006)] l1_recons: 6.9940e-02 l1_flare: 3.9013e-02 l1_base: 4.2524e-02 l1: 1.5148e-01 l_vgg: 2.7716e+00 l_vgg_base: 2.1824e+00 l_vgg_flare: 5.8919e-01 
2023-04-12 09:56:32,733 INFO: [Flare..][epoch:  0, iter:   2,100, lr:(1.000e-04,)] [eta: 12 days, 4:01:34, time (data): 0.879 (0.006)] l1_recons: 3.5124e-02 l1_flare: 3.7460e-02 l1_base: 2.9435e-02 l1: 1.0202e-01 l_vgg: 1.6733e+00 l_vgg_base: 1.1282e+00 l_vgg_flare: 5.4511e-01 
2023-04-12 09:58:00,887 INFO: [Flare..][epoch:  0, iter:   2,200, lr:(1.000e-04,)] [eta: 12 days, 4:03:40, time (data): 0.880 (0.006)] l1_recons: 5.8485e-02 l1_flare: 7.6938e-02 l1_base: 5.4605e-02 l1: 1.9003e-01 l_vgg: 1.9641e+00 l_vgg_base: 1.0389e+00 l_vgg_flare: 9.2519e-01 
2023-04-12 09:59:28,713 INFO: [Flare..][epoch:  0, iter:   2,300, lr:(1.000e-04,)] [eta: 12 days, 4:02:37, time (data): 0.879 (0.006)] l1_recons: 4.8209e-02 l1_flare: 5.7372e-02 l1_base: 3.7687e-02 l1: 1.4327e-01 l_vgg: 1.9529e+00 l_vgg_base: 1.3042e+00 l_vgg_flare: 6.4861e-01 
2023-04-12 10:00:56,743 INFO: [Flare..][epoch:  0, iter:   2,400, lr:(1.000e-04,)] [eta: 12 days, 4:03:13, time (data): 0.879 (0.006)] l1_recons: 4.7542e-02 l1_flare: 4.0845e-02 l1_base: 3.2448e-02 l1: 1.2083e-01 l_vgg: 2.5346e+00 l_vgg_base: 1.7486e+00 l_vgg_flare: 7.8603e-01 
2023-04-12 10:02:24,691 INFO: [Flare..][epoch:  0, iter:   2,500, lr:(1.000e-04,)] [eta: 12 days, 4:03:00, time (data): 0.878 (0.006)] l1_recons: 3.0208e-02 l1_flare: 8.1317e-02 l1_base: 5.1215e-02 l1: 1.6274e-01 l_vgg: 2.4021e+00 l_vgg_base: 1.5633e+00 l_vgg_flare: 8.3880e-01 
2023-04-12 10:03:52,698 INFO: [Flare..][epoch:  0, iter:   2,600, lr:(1.000e-04,)] [eta: 12 days, 4:03:08, time (data): 0.879 (0.006)] l1_recons: 4.4627e-02 l1_flare: 3.5459e-02 l1_base: 2.3591e-02 l1: 1.0368e-01 l_vgg: 2.0723e+00 l_vgg_base: 1.4795e+00 l_vgg_flare: 5.9283e-01 
2023-04-12 10:05:20,828 INFO: [Flare..][epoch:  0, iter:   2,700, lr:(1.000e-04,)] [eta: 12 days, 4:04:04, time (data): 0.881 (0.006)] l1_recons: 3.1961e-02 l1_flare: 6.5600e-02 l1_base: 4.0157e-02 l1: 1.3772e-01 l_vgg: 3.1380e+00 l_vgg_base: 1.7503e+00 l_vgg_flare: 1.3877e+00 
2023-04-12 10:06:48,894 INFO: [Flare..][epoch:  0, iter:   2,800, lr:(1.000e-04,)] [eta: 12 days, 4:04:22, time (data): 0.881 (0.006)] l1_recons: 5.0154e-02 l1_flare: 4.5418e-02 l1_base: 2.6845e-02 l1: 1.2242e-01 l_vgg: 2.0206e+00 l_vgg_base: 1.0788e+00 l_vgg_flare: 9.4179e-01 
2023-04-12 10:08:16,852 INFO: [Flare..][epoch:  0, iter:   2,900, lr:(1.000e-04,)] [eta: 12 days, 4:03:49, time (data): 0.880 (0.006)] l1_recons: 3.1205e-02 l1_flare: 4.1218e-02 l1_base: 2.6045e-02 l1: 9.8468e-02 l_vgg: 1.4356e+00 l_vgg_base: 1.0047e+00 l_vgg_flare: 4.3095e-01 
2023-04-12 10:09:44,820 INFO: [Flare..][epoch:  0, iter:   3,000, lr:(1.000e-04,)] [eta: 12 days, 4:03:15, time (data): 0.880 (0.006)] l1_recons: 5.5382e-02 l1_flare: 5.2449e-02 l1_base: 5.2661e-02 l1: 1.6049e-01 l_vgg: 3.3960e+00 l_vgg_base: 1.9332e+00 l_vgg_flare: 1.4628e+00 
2023-04-12 10:11:12,727 INFO: [Flare..][epoch:  0, iter:   3,100, lr:(1.000e-04,)] [eta: 12 days, 4:02:15, time (data): 0.880 (0.006)] l1_recons: 3.3447e-02 l1_flare: 5.1753e-02 l1_base: 3.6476e-02 l1: 1.2168e-01 l_vgg: 2.1013e+00 l_vgg_base: 1.4533e+00 l_vgg_flare: 6.4807e-01 
2023-04-12 10:12:40,762 INFO: [Flare..][epoch:  0, iter:   3,200, lr:(1.000e-04,)] [eta: 12 days, 4:02:00, time (data): 0.880 (0.006)] l1_recons: 5.8836e-02 l1_flare: 5.6130e-02 l1_base: 4.9709e-02 l1: 1.6468e-01 l_vgg: 5.2089e+00 l_vgg_base: 2.4639e+00 l_vgg_flare: 2.7450e+00 
2023-04-12 10:14:08,890 INFO: [Flare..][epoch:  0, iter:   3,300, lr:(1.000e-04,)] [eta: 12 days, 4:02:16, time (data): 0.881 (0.006)] l1_recons: 2.7123e-02 l1_flare: 3.5632e-02 l1_base: 2.3393e-02 l1: 8.6148e-02 l_vgg: 1.8304e+00 l_vgg_base: 1.0950e+00 l_vgg_flare: 7.3548e-01 
2023-04-12 10:15:36,967 INFO: [Flare..][epoch:  0, iter:   3,400, lr:(1.000e-04,)] [eta: 12 days, 4:02:07, time (data): 0.881 (0.006)] l1_recons: 2.7746e-02 l1_flare: 1.4698e-02 l1_base: 1.4628e-02 l1: 5.7071e-02 l_vgg: 1.7748e+00 l_vgg_base: 1.0488e+00 l_vgg_flare: 7.2608e-01 
2023-04-12 10:17:05,068 INFO: [Flare..][epoch:  0, iter:   3,500, lr:(1.000e-04,)] [eta: 12 days, 4:02:01, time (data): 0.880 (0.006)] l1_recons: 3.0652e-02 l1_flare: 4.0476e-02 l1_base: 2.7115e-02 l1: 9.8243e-02 l_vgg: 1.5331e+00 l_vgg_base: 9.4656e-01 l_vgg_flare: 5.8654e-01 
2023-04-12 10:18:33,239 INFO: [Flare..][epoch:  0, iter:   3,600, lr:(1.000e-04,)] [eta: 12 days, 4:02:14, time (data): 0.881 (0.006)] l1_recons: 1.8302e-02 l1_flare: 5.6109e-02 l1_base: 2.5919e-02 l1: 1.0033e-01 l_vgg: 1.6562e+00 l_vgg_base: 8.9344e-01 l_vgg_flare: 7.6275e-01 
2023-04-12 10:20:01,315 INFO: [Flare..][epoch:  0, iter:   3,700, lr:(1.000e-04,)] [eta: 12 days, 4:01:51, time (data): 0.879 (0.006)] l1_recons: 2.2353e-02 l1_flare: 6.7349e-02 l1_base: 2.7994e-02 l1: 1.1770e-01 l_vgg: 2.7049e+00 l_vgg_base: 1.6362e+00 l_vgg_flare: 1.0687e+00 
2023-04-12 10:21:29,538 INFO: [Flare..][epoch:  0, iter:   3,800, lr:(1.000e-04,)] [eta: 12 days, 4:02:11, time (data): 0.881 (0.006)] l1_recons: 2.4683e-02 l1_flare: 4.3146e-02 l1_base: 1.5848e-02 l1: 8.3677e-02 l_vgg: 2.1327e+00 l_vgg_base: 9.3827e-01 l_vgg_flare: 1.1944e+00 
2023-04-12 10:22:57,421 INFO: [Flare..][epoch:  0, iter:   3,900, lr:(1.000e-04,)] [eta: 12 days, 4:00:42, time (data): 0.880 (0.006)] l1_recons: 1.9956e-02 l1_flare: 5.1563e-02 l1_base: 2.1291e-02 l1: 9.2810e-02 l_vgg: 1.6831e+00 l_vgg_base: 9.7382e-01 l_vgg_flare: 7.0924e-01 
2023-04-12 10:24:25,591 INFO: [Flare..][epoch:  0, iter:   4,000, lr:(1.000e-04,)] [eta: 12 days, 4:00:38, time (data): 0.881 (0.006)] l1_recons: 1.4001e-02 l1_flare: 6.3809e-02 l1_base: 4.4293e-02 l1: 1.2210e-01 l_vgg: 2.8384e+00 l_vgg_base: 1.5986e+00 l_vgg_flare: 1.2398e+00 
2023-04-12 10:25:53,779 INFO: [Flare..][epoch:  0, iter:   4,100, lr:(1.000e-04,)] [eta: 12 days, 4:00:35, time (data): 0.882 (0.006)] l1_recons: 4.6376e-02 l1_flare: 5.2836e-02 l1_base: 3.7995e-02 l1: 1.3721e-01 l_vgg: 2.4180e+00 l_vgg_base: 1.3896e+00 l_vgg_flare: 1.0283e+00 
2023-04-12 10:27:21,388 INFO: [Flare..][epoch:  0, iter:   4,200, lr:(1.000e-04,)] [eta: 12 days, 3:57:43, time (data): 0.879 (0.006)] l1_recons: 3.3676e-02 l1_flare: 5.8702e-02 l1_base: 2.8522e-02 l1: 1.2090e-01 l_vgg: 2.8206e+00 l_vgg_base: 1.6032e+00 l_vgg_flare: 1.2174e+00 
2023-04-12 10:28:49,463 INFO: [Flare..][epoch:  0, iter:   4,300, lr:(1.000e-04,)] [eta: 12 days, 3:57:05, time (data): 0.882 (0.006)] l1_recons: 3.5429e-02 l1_flare: 1.8846e-02 l1_base: 2.2697e-02 l1: 7.6972e-02 l_vgg: 1.2837e+00 l_vgg_base: 1.0410e+00 l_vgg_flare: 2.4269e-01 
2023-04-12 10:30:17,687 INFO: [Flare..][epoch:  0, iter:   4,400, lr:(1.000e-04,)] [eta: 12 days, 3:57:05, time (data): 0.882 (0.006)] l1_recons: 2.0646e-02 l1_flare: 6.6012e-02 l1_base: 3.9995e-02 l1: 1.2665e-01 l_vgg: 2.2487e+00 l_vgg_base: 1.3814e+00 l_vgg_flare: 8.6732e-01 
2023-04-12 10:31:45,977 INFO: [Flare..][epoch:  0, iter:   4,500, lr:(1.000e-04,)] [eta: 12 days, 3:57:18, time (data): 0.882 (0.006)] l1_recons: 2.0993e-02 l1_flare: 4.1567e-02 l1_base: 2.0219e-02 l1: 8.2780e-02 l_vgg: 1.8731e+00 l_vgg_base: 1.0862e+00 l_vgg_flare: 7.8691e-01 
2023-04-12 10:33:13,942 INFO: [Flare..][epoch:  0, iter:   4,600, lr:(1.000e-04,)] [eta: 12 days, 3:56:03, time (data): 0.881 (0.006)] l1_recons: 2.5078e-02 l1_flare: 3.9762e-02 l1_base: 2.2776e-02 l1: 8.7616e-02 l_vgg: 1.7546e+00 l_vgg_base: 1.0988e+00 l_vgg_flare: 6.5581e-01 
2023-04-12 10:34:42,205 INFO: [Flare..][epoch:  0, iter:   4,700, lr:(1.000e-04,)] [eta: 12 days, 3:56:03, time (data): 0.882 (0.006)] l1_recons: 3.6080e-02 l1_flare: 1.0478e-01 l1_base: 4.7322e-02 l1: 1.8818e-01 l_vgg: 2.5673e+00 l_vgg_base: 1.6650e+00 l_vgg_flare: 9.0231e-01 
2023-04-12 10:36:10,291 INFO: [Flare..][epoch:  0, iter:   4,800, lr:(1.000e-04,)] [eta: 12 days, 3:55:15, time (data): 0.881 (0.006)] l1_recons: 2.7723e-02 l1_flare: 4.8175e-02 l1_base: 2.1494e-02 l1: 9.7391e-02 l_vgg: 1.8555e+00 l_vgg_base: 1.1250e+00 l_vgg_flare: 7.3044e-01 
2023-04-12 10:37:38,396 INFO: [Flare..][epoch:  0, iter:   4,900, lr:(1.000e-04,)] [eta: 12 days, 3:54:31, time (data): 0.882 (0.006)] l1_recons: 1.4613e-02 l1_flare: 4.7485e-02 l1_base: 2.1577e-02 l1: 8.3675e-02 l_vgg: 2.2094e+00 l_vgg_base: 1.2846e+00 l_vgg_flare: 9.2472e-01 
2023-04-12 10:39:06,515 INFO: [Flare..][epoch:  0, iter:   5,000, lr:(1.000e-04,)] [eta: 12 days, 3:53:47, time (data): 0.881 (0.006)] l1_recons: 1.8367e-02 l1_flare: 2.1872e-02 l1_base: 1.2303e-02 l1: 5.2543e-02 l_vgg: 2.0039e+00 l_vgg_base: 1.0632e+00 l_vgg_flare: 9.4066e-01 
