2023-04-12 06:49:27,962 INFO: 
                ____                _       _____  ____
               / __ ) ____ _ _____ (_)_____/ ___/ / __ \
              / __  |/ __ `// ___// // ___/\__ \ / /_/ /
             / /_/ // /_/ /(__  )/ // /__ ___/ // _, _/
            /_____/ \__,_//____//_/ \___//____//_/ |_|
     ______                   __   __                 __      __
    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /
   / / __ / __ \ / __ \ / __  /  / /   / / / // ___// //_/  / /
  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/
  \____/ \____/ \____/ \____/  /_____/\____/ \___//_/|_|  (_)
    
Version Information: 
	BasicSR: 1.4.1
	PyTorch: 1.9.0+cu111
	TorchVision: 0.10.0+cu111
2023-04-12 06:49:27,962 INFO: 
  name: FlareRemoval_Uformer_6ch
  model_type: DeflareModel
  scale: 1
  num_gpu: 2
  manual_seed: 0
  datasets:[
    train:[
      name: Flare7K
      type: Flare_Pair_Loader
      image_path: dataset/Flickr24K
      scattering_dict:[
        Flare7K_scattering: dataset/Flare7k/Scattering_Flare/Compound_Flare
      ]
      reflective_dict:[
        Flare7K_reflective: dataset/Flare7k/Reflective_Flare
      ]
      transform_base:[
        img_size: 512
      ]
      transform_flare:[
        scale_min: 0.8
        scale_max: 1.5
        translate: 300
        shear: 20
      ]
      mask_type: None
      use_shuffle: True
      num_worker_per_gpu: 4
      batch_size_per_gpu: 1
      dataset_enlarge_ratio: 1
      prefetch_mode: None
      phase: train
      scale: 1
    ]
    val:[
      name: flare_test
      type: Image_Pair_Loader
      dataroot_gt: dataset/Flare7k/valid/gt
      dataroot_lq: dataset/Flare7k/valid/input
      gt_size: 512
      phase: val
      scale: 1
    ]
  ]
  network_g:[
    type: Uformer
    img_size: 512
    img_ch: 3
    output_ch: 6
    multi_stage: 1
  ]
  path:[
    pretrain_network_g: None
    strict_load_g: True
    resume_state: None
    experiments_root: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch
    models: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch/models
    training_states: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch/training_states
    log: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch
    visualization: /home/fanglihuang/workspace/data/Flare7K/experiments/FlareRemoval_Uformer_6ch/visualization
  ]
  train:[
    optim_g:[
      type: Adam
      lr: 0.0001
      weight_decay: 0
      betas: [0.9, 0.99]
    ]
    scheduler:[
      type: MultiStepLR
      milestones: [200000]
      gamma: 1.0
    ]
    ema_decay: 0
    total_iter: 1200000
    warmup_iter: -1
    l1_opt:[
      type: L_Abs_pure
      loss_weight: 0.5
    ]
    perceptual:[
      type: L_percepture
      loss_weight: 0.5
    ]
  ]
  val:[
    val_freq: 5000.0
    save_img: True
    metrics:[
      psnr:[
        type: calculate_psnr
        crop_border: 4
        test_y_channel: False
      ]
      ssim:[
        type: calculate_ssim
        crop_border: 4
        test_y_channel: False
      ]
    ]
  ]
  logger:[
    print_freq: 100
    save_checkpoint_freq: 50000.0
    use_tb_logger: True
    wandb:[
      project: None
      resume_id: None
    ]
  ]
  dist_params:[
    backend: nccl
    port: 29500
  ]
  dist: False
  rank: 0
  world_size: 1
  auto_resume: False
  is_train: True
  root_path: /home/fanglihuang/workspace/data/Flare7K

2023-04-12 06:49:28,259 INFO: Dataset [Flare_Pair_Loader] - Flare7K is built.
2023-04-12 06:49:28,260 INFO: Training statistics:
	Number of train images: 23949
	Dataset enlarge ratio: 1
	Batch size per gpu: 1
	World size (gpu number): 1
	Require iter number per epoch: 23949
	Total epochs: 51; iters: 1200000.
2023-04-12 06:49:28,260 INFO: Dataset [Image_Pair_Loader] - flare_test is built.
2023-04-12 06:49:28,260 INFO: Number of val images/folders in flare_test: 0
2023-04-12 06:49:29,181 INFO: Network [Uformer] is created.
2023-04-12 06:49:32,122 INFO: Network: DataParallel - Uformer, with parameters: 20,473,888
2023-04-12 06:49:32,123 INFO: Uformer(
  embed_dim=32, token_projection=linear, token_mlp=ffn,win_size=8
  (pos_drop): Dropout(p=0.0, inplace=False)
  (input_proj): InputProj(
    (proj): Sequential(
      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LeakyReLU(negative_slope=0.01, inplace=True)
    )
  )
  (output_proj): OutputProj(
    (proj): Sequential(
      (0): Conv2d(64, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
  )
  (encoderlayer_0): BasicUformerLayer(
    dim=32, input_resolution=(512, 512), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=32, input_resolution=(512, 512), num_heads=1, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=32, input_resolution=(512, 512), num_heads=1, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=32, win_size=(8, 8), num_heads=1
          (qkv): LinearProjection(
            (to_q): Linear(in_features=32, out_features=32, bias=True)
            (to_kv): Linear(in_features=32, out_features=64, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=32, out_features=32, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.014)
        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=32, out_features=128, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=128, out_features=32, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_0): Downsample(
    (conv): Sequential(
      (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_1): BasicUformerLayer(
    dim=64, input_resolution=(256, 256), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(256, 256), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.029)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(256, 256), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.043)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_1): Downsample(
    (conv): Sequential(
      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(128, 128), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.057)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(128, 128), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.071)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_2): Downsample(
    (conv): Sequential(
      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (encoderlayer_3): BasicUformerLayer(
    dim=256, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(64, 64), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.086)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(64, 64), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (dowsample_3): Downsample(
    (conv): Sequential(
      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
  (conv): BasicUformerLayer(
    dim=512, input_resolution=(32, 32), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(32, 32), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(32, 32), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_0): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_0): BasicUformerLayer(
    dim=512, input_resolution=(64, 64), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=512, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.100)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=512, input_resolution=(64, 64), num_heads=16, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=512, win_size=(8, 8), num_heads=16
          (qkv): LinearProjection(
            (to_q): Linear(in_features=512, out_features=512, bias=True)
            (to_kv): Linear(in_features=512, out_features=1024, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=512, out_features=512, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.086)
        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_1): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(512, 128, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_1): BasicUformerLayer(
    dim=256, input_resolution=(128, 128), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=256, input_resolution=(128, 128), num_heads=8, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.071)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=256, input_resolution=(128, 128), num_heads=8, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=256, win_size=(8, 8), num_heads=8
          (qkv): LinearProjection(
            (to_q): Linear(in_features=256, out_features=256, bias=True)
            (to_kv): Linear(in_features=256, out_features=512, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=256, out_features=256, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.057)
        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_2): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_2): BasicUformerLayer(
    dim=128, input_resolution=(256, 256), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=128, input_resolution=(256, 256), num_heads=4, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.043)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=128, input_resolution=(256, 256), num_heads=4, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=128, win_size=(8, 8), num_heads=4
          (qkv): LinearProjection(
            (to_q): Linear(in_features=128, out_features=128, bias=True)
            (to_kv): Linear(in_features=128, out_features=256, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=128, out_features=128, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.029)
        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=128, out_features=512, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=512, out_features=128, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (upsample_3): Upsample(
    (deconv): Sequential(
      (0): ConvTranspose2d(128, 32, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (decoderlayer_3): BasicUformerLayer(
    dim=64, input_resolution=(512, 512), depth=2
    (blocks): ModuleList(
      (0): LeWinTransformerBlock(
        dim=64, input_resolution=(512, 512), num_heads=2, win_size=8, shift_size=0, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): DropPath(drop_prob=0.014)
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): LeWinTransformerBlock(
        dim=64, input_resolution=(512, 512), num_heads=2, win_size=8, shift_size=4, mlp_ratio=4.0
        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (attn): WindowAttention(
          dim=64, win_size=(8, 8), num_heads=2
          (qkv): LinearProjection(
            (to_q): Linear(in_features=64, out_features=64, bias=True)
            (to_kv): Linear(in_features=64, out_features=128, bias=True)
          )
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=64, out_features=64, bias=True)
          (se_layer): Identity()
          (proj_drop): Dropout(p=0.0, inplace=False)
          (softmax): Softmax(dim=-1)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=64, out_features=256, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=256, out_features=64, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
  )
  (activation): Sequential(
    (0): Sigmoid()
  )
)
2023-04-12 06:49:32,125 INFO: Loss [L_Abs_pure] is created.
2023-04-12 06:49:33,916 INFO: Loss [L_percepture] is created.
2023-04-12 06:49:33,919 INFO: Model [DeflareModel] is created.
2023-04-12 06:49:34,223 INFO: Start training from epoch: 0, iter: 0
2023-04-12 06:51:07,971 INFO: [Flare..][epoch:  0, iter:     100, lr:(1.000e-04,)] [eta: 11 days, 11:55:18, time (data): 0.937 (0.048)] l1_recons: 2.7010e-01 l1_flare: 1.4885e-01 l1_base: 9.4150e-02 l1: 5.1311e-01 l_vgg: 5.3757e+00 l_vgg_base: 4.1847e+00 l_vgg_flare: 1.1910e+00 
2023-04-12 06:52:34,114 INFO: [Flare..][epoch:  0, iter:     200, lr:(1.000e-04,)] [eta: 11 days, 17:28:10, time (data): 0.899 (0.027)] l1_recons: 1.6212e-01 l1_flare: 7.7496e-02 l1_base: 7.0215e-02 l1: 3.0983e-01 l_vgg: 6.2539e+00 l_vgg_base: 4.9944e+00 l_vgg_flare: 1.2595e+00 
2023-04-12 06:54:01,957 INFO: [Flare..][epoch:  0, iter:     300, lr:(1.000e-04,)] [eta: 11 days, 21:11:51, time (data): 0.878 (0.006)] l1_recons: 2.5549e-01 l1_flare: 7.6618e-02 l1_base: 1.0856e-01 l1: 4.4067e-01 l_vgg: 4.6702e+00 l_vgg_base: 3.3972e+00 l_vgg_flare: 1.2731e+00 
2023-04-12 06:55:29,912 INFO: [Flare..][epoch:  0, iter:     400, lr:(1.000e-04,)] [eta: 11 days, 23:08:47, time (data): 0.879 (0.006)] l1_recons: 1.7632e-01 l1_flare: 6.7254e-02 l1_base: 8.5369e-02 l1: 3.2894e-01 l_vgg: 4.6293e+00 l_vgg_base: 3.9479e+00 l_vgg_flare: 6.8147e-01 
2023-04-12 06:56:57,745 INFO: [Flare..][epoch:  0, iter:     500, lr:(1.000e-04,)] [eta: 12 days, 0:13:37, time (data): 0.879 (0.007)] l1_recons: 7.8107e-02 l1_flare: 5.7108e-02 l1_base: 5.1202e-02 l1: 1.8642e-01 l_vgg: 2.9276e+00 l_vgg_base: 2.2885e+00 l_vgg_flare: 6.3910e-01 
2023-04-12 06:58:25,635 INFO: [Flare..][epoch:  0, iter:     600, lr:(1.000e-04,)] [eta: 12 days, 0:58:16, time (data): 0.879 (0.007)] l1_recons: 7.0979e-02 l1_flare: 7.1062e-02 l1_base: 7.0939e-02 l1: 2.1298e-01 l_vgg: 3.5988e+00 l_vgg_base: 2.7515e+00 l_vgg_flare: 8.4730e-01 
2023-04-12 06:59:53,568 INFO: [Flare..][epoch:  0, iter:     700, lr:(1.000e-04,)] [eta: 12 days, 1:30:59, time (data): 0.879 (0.007)] l1_recons: 6.9132e-02 l1_flare: 5.5155e-02 l1_base: 4.1131e-02 l1: 1.6542e-01 l_vgg: 2.5706e+00 l_vgg_base: 2.0379e+00 l_vgg_flare: 5.3274e-01 
2023-04-12 07:01:21,480 INFO: [Flare..][epoch:  0, iter:     800, lr:(1.000e-04,)] [eta: 12 days, 1:54:39, time (data): 0.879 (0.007)] l1_recons: 5.3884e-02 l1_flare: 5.0605e-02 l1_base: 3.6957e-02 l1: 1.4145e-01 l_vgg: 2.5418e+00 l_vgg_base: 1.9344e+00 l_vgg_flare: 6.0745e-01 
2023-04-12 07:02:48,863 INFO: [Flare..][epoch:  0, iter:     900, lr:(1.000e-04,)] [eta: 12 days, 2:00:59, time (data): 0.874 (0.006)] l1_recons: 5.7374e-02 l1_flare: 7.3279e-02 l1_base: 6.3335e-02 l1: 1.9399e-01 l_vgg: 3.2042e+00 l_vgg_base: 2.3466e+00 l_vgg_flare: 8.5752e-01 
2023-04-12 07:04:16,592 INFO: [Flare..][epoch:  0, iter:   1,000, lr:(1.000e-04,)] [eta: 12 days, 2:12:41, time (data): 0.875 (0.006)] l1_recons: 7.0119e-02 l1_flare: 6.8066e-02 l1_base: 4.6467e-02 l1: 1.8465e-01 l_vgg: 2.7170e+00 l_vgg_base: 1.8307e+00 l_vgg_flare: 8.8635e-01 
2023-04-12 07:05:44,111 INFO: [Flare..][epoch:  0, iter:   1,100, lr:(1.000e-04,)] [eta: 12 days, 2:18:11, time (data): 0.876 (0.007)] l1_recons: 5.3811e-02 l1_flare: 2.7395e-02 l1_base: 2.9638e-02 l1: 1.1084e-01 l_vgg: 3.0384e+00 l_vgg_base: 1.9542e+00 l_vgg_flare: 1.0842e+00 
2023-04-12 07:07:11,990 INFO: [Flare..][epoch:  0, iter:   1,200, lr:(1.000e-04,)] [eta: 12 days, 2:28:30, time (data): 0.877 (0.007)] l1_recons: 9.5677e-02 l1_flare: 2.5182e-02 l1_base: 5.2854e-02 l1: 1.7371e-01 l_vgg: 3.2687e+00 l_vgg_base: 1.9107e+00 l_vgg_flare: 1.3580e+00 
2023-04-12 07:08:39,589 INFO: [Flare..][epoch:  0, iter:   1,300, lr:(1.000e-04,)] [eta: 12 days, 2:32:44, time (data): 0.876 (0.007)] l1_recons: 4.3788e-02 l1_flare: 6.5635e-02 l1_base: 4.7375e-02 l1: 1.5680e-01 l_vgg: 2.8458e+00 l_vgg_base: 1.7597e+00 l_vgg_flare: 1.0861e+00 
2023-04-12 07:10:07,611 INFO: [Flare..][epoch:  0, iter:   1,400, lr:(1.000e-04,)] [eta: 12 days, 2:42:09, time (data): 0.878 (0.007)] l1_recons: 4.2307e-02 l1_flare: 4.6898e-02 l1_base: 3.5074e-02 l1: 1.2428e-01 l_vgg: 2.4437e+00 l_vgg_base: 1.6373e+00 l_vgg_flare: 8.0638e-01 
2023-04-12 07:11:35,486 INFO: [Flare..][epoch:  0, iter:   1,500, lr:(1.000e-04,)] [eta: 12 days, 2:48:12, time (data): 0.879 (0.007)] l1_recons: 5.8686e-02 l1_flare: 4.9068e-02 l1_base: 3.4019e-02 l1: 1.4177e-01 l_vgg: 2.3263e+00 l_vgg_base: 1.5452e+00 l_vgg_flare: 7.8110e-01 
2023-04-12 07:13:03,225 INFO: [Flare..][epoch:  0, iter:   1,600, lr:(1.000e-04,)] [eta: 12 days, 2:51:35, time (data): 0.878 (0.007)] l1_recons: 4.2610e-02 l1_flare: 5.9399e-02 l1_base: 4.9265e-02 l1: 1.5127e-01 l_vgg: 2.7251e+00 l_vgg_base: 1.9510e+00 l_vgg_flare: 7.7408e-01 
2023-04-12 07:14:31,318 INFO: [Flare..][epoch:  0, iter:   1,700, lr:(1.000e-04,)] [eta: 12 days, 2:58:33, time (data): 0.880 (0.007)] l1_recons: 3.2562e-02 l1_flare: 1.8428e-02 l1_base: 1.9689e-02 l1: 7.0679e-02 l_vgg: 1.6085e+00 l_vgg_base: 1.2234e+00 l_vgg_flare: 3.8508e-01 
2023-04-12 07:15:59,410 INFO: [Flare..][epoch:  0, iter:   1,800, lr:(1.000e-04,)] [eta: 12 days, 3:04:36, time (data): 0.881 (0.007)] l1_recons: 5.7488e-02 l1_flare: 2.9454e-02 l1_base: 3.2001e-02 l1: 1.1894e-01 l_vgg: 2.7972e+00 l_vgg_base: 1.8090e+00 l_vgg_flare: 9.8815e-01 
